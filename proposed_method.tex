\section{Preliminaries}

In this section, first, we formulate the problem of Wild Unsupervised Domain Adaptation (WUDA), and then describe the base networks required to understand the proposed approach.

\subsection{Problem Formulation}
In this work, we are exploring the problem of wild unsupervised domain adaptation(WUDA) in which the target domain is fully unlabeled and the source domain is partially corrupted with label noise. The wild unsupervised domain adaptation scenario constitutes a labeled source domain ${D}_s = \{({x_i}^s, {y_i}^s)\}_{i=1}^{n_s}$ and an unlabeled target domain ${D}_t = \{({x_i}^t)\}_{i=1}^{n_t}$, while the source and target domains follow different distributions $p \neq q$. Here, we relax the assumptions of clean labels of source domain data.  Our goal is to train a deep network using noisy source data and unlabeled target which generalizes well on target data. Standard DA methods results in severe negative transfer. So we have to modify standard DA methods to eliminate the negative influence of noisy source samples and enable positive transfer of noiseless source samples.

Our aim is to train a deep learning model which is robust to both noisy source samples and distribution shift. 
To address the distribution shift, we use the base network as the domain adversarial neural network (DANN) described above, though other base networks can also be utilised for the same.
To account for the label noise in the source samples, we propose to incorporate a very well established approach for handling noise in image classification network, namely co-teaching framework [].
Before we decribe the proposed approach, we will describe the base DANN network and also the co-teaching framework for image classification setup.

\subsection{Base Network - Domain Adversarial Neural Network}
Domain adaptation is usually reduced to matching the feature distributions of the source and target domains. In the deep learning regime, this can be done by
learning new feature representations such that the source and target domains
are not distinguishable by a domain discriminator. This idea leads to the a series
of domain adversarial neural networks (DANN)\cite{uda,dann,deeptransfer}, achieving strong performance
in standard domain adaptation with shared label space across domains.
More formally, DANN is a two-player minimax game, where the first player is
a domain discriminator $G_d$ trained to distinguish the source domain from the
target domain, and the second player is a feature extractor $G_f$ simultaneously
trained to confuse the domain discriminator. $G_y$ is the classifier to classify features extracted from $G_f$.
In order to extract domain invariant and discriminant features from $G_f$, the parameters $w_f$ of feature extractor $G_f$ are learnt by maximizing the domain loss and minimizing the classification loss of source domain instances, while the parameters $w_d$ of domain discriminator are learnt by minimizing the domain loss. The parameters $w_y$ of classifier are learnt by minimizing the classification loss of source domain data.
The overall objective of the Domain Adversarial Neural Network (DANN) is
\begin{equation}
\begin{align}
    \mathcal{L}_y(G_f, G_y, D_s)  = \dfrac{1}{n_{s}} \mathlarger\sum\limits_{x_i \in D_s} L_y(G_y(G_f(x_i)),y_i)\\
     \mathcal{L}_d(G_f, G_d, D_s \cup D_t) = \dfrac{1}{n_s + n_t} \sum\limits_{x_i \in {D_s \cup D_t}} L_d(G_d(G_f(x_i)),d_i)\\
     L(w_f, w_y, w_d) = \mathcal{L}_y(G_f, G_y, D_s) - \lambda \mathcal{L}_d(G_f, G_d, D_s \cup D_t)
\end{align}
\end{equation}
$L_y$ and $L_d$ are classification loss and domain loss respectively. where $d_i$ is the domain label of $x_i$ , and $\lambda$ is a hyper-parameter to trade off the
two objectives $L_y$ and $L_d$. After training convergence:
\begin{center}
    $\hat{w}_f, \hat{w}_y  = \argmin \limits_{w_f, w_y} L(w_f, w_y, w_d)$\\
    $\hat{w}_d  = \argmax \limits_{w_d} L(w_f, w_y, w_d)$
\end{center}
\input{coteaching}

% *************************************************************

\section{Proposed CT-DANN for WUDA}
Here, we propose a novel framework, termed CT-DANN, which seamlessly integrates the co-teaching framework with the base DANN for handling noisy data in the source domain as well as the distribution shift between the source and target domain.

A flow-chart of the proposed approach is shown in Figure [].
\input{fig}
\input{net1}

The proposed CT-DANN consists of two pairs of feature extractors and classifiers, denoted by $(G_{f}^1$ \& $G_{y}^1)$ and $(G_{f}^2$ \& $G_{y}^2)$ having parameters $(w_{f}^1$ \& $w_{y}^1)$, $(w_{f}^2$ \& $w_{y}^2)$ respectively. 
Each pair of feature extractor and classifier accounts for the two networks as per the co-teaching framework.
Lets denote the networks by $P, Q$ (refer figure \ref{networks}). 

\color{red} Describe the remaining part of the network. \color{black}

Mini batches are formed from noisy source data and target data$(\mathcal{D}_s, \mathcal{D}_t)$ in step 5. Some instances in $\mathcal{D}_s$ are corrupted by label noise. Applying standard DANN results in severe negative transfer. To mitigate this negative transfer, we apply co-teaching framework to select out small loss instances from $\mathcal{D}_s$ and update the parameters of the network in standard DANN framework setting.\\
Standard co-teaching has a hard assumption of knowing the noise level i.e. $\epsilon$, but for most of the practical scenario we might not know the noise level. So instead of selecting out small loss instance and rejecting large loss instances in every mini batch, we suitably weigh them with weighting factor. Weighting should be inversely proportional to loss of the particular instance.

\subsubsection{Calculation of weighting factor}
Weights are calculated for every sample in every mini-batch to weigh their corresponding loss. Since source data is noisy, so loss corresponding to every source sample will be weighed as per their weighting factor. Domain labels for source domain instance are taken to be $1$.

\begin{equation*}
    loss' = -\log(G_y(G_f(x_i)) \quad \forall x_i \in \mathcal{D}_s
\end{equation*}
Since weights should be inversely proporational to loss, we have taken $e^{-loss}$ to be the weighting factor. Where $e$ is the base of $\log$ and it is $2$
\begin{equation}
\label{eq:weight'}
\begin{align}
    e^{-loss'} = G_y(G_f(x_i)\\
    w_{i}' =     G_y(G_f(x_i)
\end{align}
\end{equation}
We also put upper and lower thresholds on this weights i.e. for all source sample having weight (calculated as per equation \ref{eq:weight'}) greater than upper threshold are assumed to be pure and given weight of $1$ and source samples having weights lesser than lower threshold are assumed to noisy and given weight of $0$. It will be as follows:
\begin{equation*}
                w_{i}' = \begin{cases}
		            0 & if\ \quad G_y(G_f(x_i) \le lower \\
		            1 & elseif\ \quad G_y(G_f(x_i) \ge upper \\
		            G_y(G_f(x_i) & else\ 
		            \end{cases} \quad \forall x_i \in \mathcal{D}_s
\end{equation*}
In domain adaptation scenario, we have to take care of whether a particular sample is transferable or not i.e. in the feature space how close source sample is to target sample. It can be quantified by calculating domain loss.
\begin{equation}
    loss = -\log(G_y(G_f(x_i)) - k \log(1 - G_d(G_f(x_i)) \quad \forall x_i \in \mathcal{D}_s
\end{equation}
The added loss term $(-\log(1 - G_d(G_f(x_i)))$ is the quantification of how close source sample is to target sample in the feature space, if it is close to target, then it will be low otherwise it will be high. $k$ is the hyperparameter and to be tuned properly, so that one factor doesn't outweigh the other.
\begin{equation}
\begin{align}
    e^{-loss} = G_y(G_f(x_i)(1 - G_d(G_f(x_i))^k\\
    w_{i} =     G_y(G_f(x_i)(1 - G_d(G_f(x_i))^k
\end{align}
\end{equation}

\begin{equation}
\label{eq:weight}
                w_{i} = \begin{cases}
		            0 & if\ \quad G_y(G_f(x_i) \le lower \\
		            (1 - G_d(G_f(x_i))^k & elseif\ \quad G_y(G_f(x_i) \ge upper \\
		            G_y(G_f(x_i)(1 - G_d(G_f(x_i))^k & else\ 
		            \end{cases} \quad \forall x_i \in \mathcal{D}_s
\end{equation}
Weights are calculated corresponding to both the networks i.e. $P$ \& $Q$. Weights corresponding to network 1 $(P)$ and network 2 $(Q)$ be denoted $w_{i}^1$ and $w_{i}^2$ respectively. Respective feature extractors and classifiers are used in the calculation of weights corresponding to each network.  


% *************************************************************

\section{Proposed Approach for WUDA}
We trained the network described in Figure \ref{flowchart}. Domain loss and entropy loss is backpropagated through only network 1 $(P)$. For initial few epochs, the networks $P$ (resp. $Q$) selects a small proportion of instances controlled by $R(T)$ from this mini-batch $\mathcal{D}_{s}^1$ (resp. $\mathcal{D}_{s}^2$) that have small training loss (step 4 \& 5 of Algorithm \ref{algo: cal_weight}). These small loss instances are given weights of $1$ and rest instances in a mini batch are given weights of $0$. After epoch $T_{start}$, all samples in a mini batch are weighted by the formula given in equation \ref{eq:weight}. To calculate the value of $R(T)$ before epoch $T_{start}$, we take $\epsilon$ to be $0.5$ assuming noise would be less than $50\%$.\\
Weights are calculated corresponding to both the networks as described in Algorithm \ref{algo: cal_weight}. Weights will be calculated for each sample of the source mini batch $(\mathcal{D}_s)$. For example: $w_{i}^1$ is the weight of the $i^{th}$ sample in $\mathcal{D}_s$ corresponding to network 1 $(P)$ and $w_{i}^2$ be the weight corresponding to network 2 $(Q)$.
\begin{equation}
    W^j = \begin{bmatrix} w_{1}^j & w_{2}^j & \dots & w_{nb}^j \end{bmatrix}  \quad j \in \{1, 2\}
\end{equation}
Where $W^j$ is weight vector for a source mini batch; $j$ refers to the network. $nb$ is length of the source mini batch $(\mathcal{D}_s)$.

\begin{algorithm}[H]
	\caption{Calculate Weights}
	\begin{algorithmic}[1]
	    \State \textbf{Input -} Source mini batch : $\mathcal{D}_s$, Mini batch size: $nb$, epoch: $T$, $R(T)$, $T_{start}$,  Feature Extractors: $G_{f}^1$ and $G_{f}^2$(with parameters $w_{f}^1$ and $w_{f}^2$), Classifiers: $G_{y}^1$ and $G_{y}^2$(with parameters $w_{y}^1$ and $w_{y}^2$), Domain Discriminator: $G_d$ (with parameters $w_d$);
	    \State \textbf{Initialize:} $W^j = \begin{bmatrix} w_{1}^j & w_{2}^j & \dots & w_{nb}^j \end{bmatrix} =  \begin{bmatrix} 0 & 0 & \dots & 0 \end{bmatrix} \quad j \in \{1, 2\} $
		\If{$T \leq T_{start}$}
		\State \textbf{Obtain} $\mathcal{D}_{s}^1 = \argmin_{\mathcal{D}':|\mathcal{D}'|\geq R(T)|\mathcal{D}_{s}|} \mathcal{L}(G_{f}^1, G_{y}^1, \mathcal{D}')$ \Comment{($||$: Cardinality)}
	    \State \textbf{Obtain} $\mathcal{D}_{s}^2 = \argmin_{\mathcal{D}':|\mathcal{D}'|\geq R(T)|\mathcal{D}_{s}|} \mathcal{L}(G_{f}^2, G_{y}^2, \mathcal{D}')$
		
		\State \textbf{Update} $w_{i}^1 = 1 \quad if(x_i \in \mathcal{D}_{s}^1) \quad \forall x_i \in \mathcal{D}_s$
		\State \textbf{Update} $w_{i}^2 = 1 \quad if(x_i \in \mathcal{D}_{s}^2) \quad \forall x_i \in \mathcal{D}_s$
		\State \textbf{Obtain} $W^j = \begin{bmatrix} w_{1}^j & w_{2}^j & \dots & w_{nb}^j \end{bmatrix} \quad j \in \{1, 2\} $
		\EndIf
		\If{$T \textgreater T_{start}$}
		\State \textbf{Calculate} $W^1$ using equation: \ref{eq:weight} with $G_{f}^1$, $G_{y}^1$ and $G_d$
		\State \textbf{Calculate} $W^2$ using equation: \ref{eq:weight} with $G_{f}^2$, $G_{y}^2$ and $G_d$
		\EndIf
		\State \textbf{return} $W^1$, $W^2$
	\end{algorithmic} 
\label{algo: cal_weight}
\end{algorithm}

After the calculation of weights $(W^1, W^2)$, it will be cross propagated in the peer networks to calculate the classification loss $(\mathcal{L}_{y}^1, \mathcal{L}_{y}^2)$ as shown in the Figure \ref{flowchart}. Instances which have smaller loss (i.e. higher weight) are assumed to less noisy than instances having larger loss (i.e. lesser weight). This ensures that higher importance is given to the less noisy data to update the parameters of the $P$ \& $Q$.\\
But only this does not tackle domain shift. To address this, we use domain adversarial learning\cite{dann} to learn transferable features in a two-player minimax game: the first player is a domain discriminator $G_d$ trained to distinguish the feature representations of the source domain from the target domain, and the second player is a feature extractor $G_{f}^1$ trained simultaneously to deceive the domain discriminator. $G_{f}^1$ and $G_d$ are involved in two involved in two player minimax game.

\begin{algorithm}[H]
	\caption{Modified Algorithm.} 
	\begin{algorithmic}[1]
	    \State \textbf{Input -} Feature Extractors: $G_{f}^1$ and $G_{f}^2$(with parameters $w_{f}^1$ and $w_{f}^2$), Classifiers: $G_{y}^1$ and $G_{y}^2$(with parameters $w_{y}^1$ and $w_{y}^2$), Domain Discriminator: $G_d$ (with parameters $w_d$), learning rate $\eta$, fixed $\epsilon$, epoch $T_k$ and $T_{max}$, iteration $N_{max}$;
		\For {$T=1,2,\ldots, T_{max}$}
		    \State Source training set ${D_s}$, Target Training set ${D_t}$
			    \For {$N=1,2,\ldots, N_{max}$}
				    \State \textbf{Fetch} mini batch $\mathcal{D}_s$, $\mathcal{D}_t$ from ${D_s}$, ${D_t}$ respectively.
				    \State \textbf{Obtain} $W^1, W^2 = \text{Calculate Weights}$ \Comment{Algorithm \ref{algo: cal_weight}}
				    
				    \State \textbf{Obtain} $\mathcal{L}_{y}^1(G_{f}^1, G_{y}^1,W^2) =\dfrac{1}{|\mathcal{D}_{s}|} \mathlarger\sum\limits_{x_i \in \mathcal{D}_{s}} w_{i}^2 L_y(G_{y}^1(G_{f}^1(x_i)),y_i) \quad i \in [1, nb]$
				    
                    \State \textbf{Obtain} $\mathcal{L}_{y}^2(G_{f}^2, G_{y}^2, W^1) =\dfrac{1}{|\mathcal{D}_{s}^1|} \mathlarger\sum\limits_{x_i \in \mathcal{D}_{s}^1} w_{i}^1 L_y(G_{y}^2(G_{f}^2(x_i)),y_i) \quad i \in [1, nb]$
				    
				    \State \textbf{Obtain} $\mathcal{L}_d(G_{f}^1, G_{d}, \mathcal{D}_s \cup \mathcal{D}_t) = \dfrac{1}{|\mathcal{D}_s| + |\mathcal{D}_t|} \mathlarger\sum\limits_{x_i \in \mathcal{D}_s \cup \mathcal{D}_t} L_d(G_d(G_{f}^1(x_i)), d_i)$
				    
				    \State \textbf{Obtain} $\mathcal{L}_t(G_{f}^1, G_{y}^1, \mathcal{D}_t) = - \dfrac{1}{|\mathcal{D}_t|} \mathlarger\sum\limits_{x_i \in \mathcal{D}_t} H_y(G_{y}^1(G_{f}^1(x_i)))$ \Comment{ $H_y$: Entropy}
				    
				    \State \textbf{Obtain} $Loss_1(w_{f}^1, w_{y}^1, w_{d}^1) = \mathcal{L}_{y}^1 + \lambda_1 \mathcal{L}_{d} + \lambda_2\mathcal{L}_{t}^1$
				    
				    \State \textbf{Obtain} $Loss_2(w_{f}^2, w_{y}^2) = \mathcal{L}_{y}^2$
				    
				    \State \textbf{Update} $w_{f}^1 = w_{f}^1 - \eta\nabla Loss_1(w_{f}^1)$
				    
				    \State \textbf{Update} $w_{y}^1 = w_{y} - \eta\nabla Loss_1(w_{y}^1)$
				    
				    \State \textbf{Update} $w_{d} = w_{d} - \eta\nabla Loss_1(w_{d})$
				    
				    \State \textbf{Update} $w_{f}^2 = w_{f}^2 - \eta\nabla Loss_2(w_{f}^2)$
				    
				    \State \textbf{Update} $w_{y}^2 = w_{y}^2 - \eta\nabla Loss_2(w_{y}^2)$
			    \EndFor
			 \State \textbf{Update} $R(T)= 1- min\bigg\{\dfrac{T}{T_k}\epsilon, \epsilon\bigg\}$
	\EndFor
	\end{algorithmic} 
\end{algorithm}