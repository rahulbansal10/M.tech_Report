\section{Related Work}
In particular, one common deep solution for unsupervised domain adaptation is to guide the feature learning in DCNNs by minimizing the domain discrepancy with Maximum Mean Discrepancy (MMD). MMD is an effective non-parametric metric for the comparisons between the distributions of source and target domains. \cite{mmd} is one of early works that incorporates MMD into DCNNs with regular supervised classification loss on source domain to learn both semantically meaningful and domain invariant representation. MMD based methods \cite{mmd,ltdan} transfers deep convolutional networks (DCNNs) by adding adaptation layers through which the kernel embeddings of distributions are matched by minimizing MMD. Residual transfer network \cite{rtn} improves the MMD-based methods by adding a shortcut path and adopting entropy minimization criterion. 
Another direction is by exploiting a domain discriminator to distinguish the source and target while learning deep features to confuse the discriminator in an adversarial training paradigm \cite{uda, dann, deeptransfer}.\\
These all methods assume a clean source domain which is limited and expensive in many real-world applications. State-of-the-art domain adaptation methods may suffer from negative transfer caused by noisy source data in wild unsupervised domain adaptation, which will deteriorate the generalization power of networks trained on the noisy source domain when applied to the target domain.\\
Learning discriminative models from datasets with noisy labels is an active area of research. \cite{DBLP:journals/corr/ZhangBHRV16} empirically demonstrated that noisy labels will be memorized by DNNs which destroys their generalization capability. Transferable Curriculum Learning (TCL) \cite{tcl} is one of the state art approach dealing with WUDA setting. Authors in TCL paper used curricum learning \cite{curricumlearn_latent, curricumlearn} which organizes examples in a meaningful order to
promote convergence and optimization. It prioritizes easier examples of smaller loss by
assigning higher weights to them. Different from all previous works, here we incorporate state of the art approach: Co-teaching \cite{coteaching} to learn classifiers from noisy data into standard domain adaptation network. Our aim is to modify the existing standard domain adaptation network to deal with WUDA problem setting also. 